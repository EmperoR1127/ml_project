{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmperoR1127/ml_project/blob/emperor/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_TbqKhLg9Yf",
        "colab_type": "code",
        "outputId": "5b4deceb-7e76-4139-f414-b0593aa6b784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6GJppz9hBv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \"/content/drive/My Drive/\"\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True):\n",
        "    path = os.path.join(PROJECT_ROOT_DIR, \"Images\", fig_id + \".png\")\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOO7V1uePIVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the dataset\n",
        "path = \"/content/drive/My Drive/Data/H-1B_Disclosure_RAW_Data.csv\"\n",
        "df = pd.read_csv(path, encoding='utf-8')\n",
        "processed_data = df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlltPJJUAauj",
        "colab_type": "text"
      },
      "source": [
        "#Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzSJA1Fa0ZLD",
        "colab_type": "text"
      },
      "source": [
        "#####Drop correlated columns and create new columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgedKr4jm9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_data = processed_data.drop([\"CASE_NUMBER\", \"VISA_CLASS\", \n",
        "                                        \"EMPLOYER_NAME\", \"EMPLOYER_STATE\",\"EMPLOYER_POSTAL_CODE\", \n",
        "                                        \"EMPLOYER_CITY\", \"EMPLOYER_BUSINESS_DBA\", \n",
        "                                        \"EMPLOYER_COUNTRY\", \"EMPLOYER_PROVINCE\", \"EMPLOYER_ADDRESS\", \n",
        "                                        \"EMPLOYER_PHONE\", \"EMPLOYER_PHONE_EXT\", \n",
        "                                        \"AGENT_ATTORNEY_NAME\", \"AGENT_ATTORNEY_CITY\", \"AGENT_ATTORNEY_STATE\",\n",
        "                                        \"JOB_TITLE\", \"SOC_NAME\",\n",
        "                                        \"PW_SOURCE\", \"PW_SOURCE_YEAR\", \"PW_SOURCE_OTHER\", \"WAGE_RATE_OF_PAY_FROM\",\n",
        "                                        \"WAGE_RATE_OF_PAY_TO\", \"WAGE_UNIT_OF_PAY\",\n",
        "                                        \"WORKSITE_CITY\", \"WORKSITE_COUNTY\", \"WORKSITE_POSTAL_CODE\", \n",
        "                                        \"ORIGINAL_CERT_DATE\", \"PUBLIC_DISCLOSURE_LOCATION\"], axis=1)\n",
        "#format EMPLOYMENT_START_DATE and EMPLOYMENT_END_DATE\n",
        "processed_data['CASE_SUBMITTED'] = pd.to_datetime(processed_data['CASE_SUBMITTED'],infer_datetime_format=True,errors='coerce')\n",
        "processed_data['DECISION_DATE'] = pd.to_datetime(processed_data['DECISION_DATE'],infer_datetime_format=True,errors='coerce')\n",
        "processed_data['EMPLOYMENT_START_DATE'] = pd.to_datetime(processed_data['EMPLOYMENT_START_DATE'],infer_datetime_format=True,errors='coerce')\n",
        "processed_data['EMPLOYMENT_END_DATE'] = pd.to_datetime(processed_data['EMPLOYMENT_END_DATE'],infer_datetime_format=True,errors='coerce')\n",
        "#drop NaT rows because we can't \"guess\" the specific date\n",
        "processed_data = processed_data[processed_data.CASE_SUBMITTED != 'NaT']\n",
        "processed_data = processed_data[processed_data.DECISION_DATE != 'NaT']\n",
        "processed_data = processed_data[processed_data.EMPLOYMENT_START_DATE != 'NaT']\n",
        "processed_data = processed_data[processed_data.EMPLOYMENT_END_DATE != 'NaT']\n",
        "#add one column as EMP_PERIOD, and drop EMPLOYMENT_START_DATE and EMPLOYMENT_END_DATE\n",
        "processed_data['EMP_PERIOD'] = processed_data['EMPLOYMENT_END_DATE'] - processed_data['EMPLOYMENT_START_DATE']\n",
        "processed_data['EMP_PERIOD'] = processed_data['EMP_PERIOD']/np.timedelta64(1,'Y')\n",
        "#train_set = train_set[train_set.EMP_PERIOD != '-']\n",
        "processed_data['EMP_PERIOD'] = processed_data['EMP_PERIOD'].astype(float)\n",
        "#add one column as PROCESS_TIME, indicating processing time of visa application\n",
        "processed_data['PROCESS_TIME'] = processed_data['DECISION_DATE'] - processed_data['CASE_SUBMITTED']\n",
        "processed_data['PROCESS_TIME'] = processed_data['PROCESS_TIME'].map(lambda x: str(x)[:1])\n",
        "processed_data['PROCESS_TIME'] = processed_data['PROCESS_TIME'].astype(float)\n",
        "processed_data = processed_data.drop([\"EMPLOYMENT_START_DATE\", \"EMPLOYMENT_END_DATE\"], axis=1)\n",
        "processed_data = processed_data.drop([\"CASE_SUBMITTED\", \"DECISION_DATE\"], axis=1)\n",
        "\n",
        "#concatenate the first 2 digit of column SOC_CODE and NAIC_CODE\n",
        "processed_data['SOC_CODE'] = processed_data['SOC_CODE'].map(lambda x: str(x)[:2])\n",
        "processed_data['NAICS_CODE'] = processed_data['NAICS_CODE'].map(lambda x: str(x)[:2])\n",
        "#remove impurity in the column\n",
        "processed_data = processed_data[processed_data.PW_UNIT_OF_PAY != 'N']\n",
        "processed_data = processed_data[processed_data.PREVAILING_WAGE != 'N']\n",
        "#according to google, there are 2080 working hours per year\n",
        "pw_unit_column = {\"Year\":1, \"Hour\":2080, \"Month\":12, \"Week\":52, \"Bi-Weekly\":26}\n",
        "processed_data['PW_UNIT_OF_PAY'] = processed_data['PW_UNIT_OF_PAY'].replace(pw_unit_column)\n",
        "#remove ',' in the column value\n",
        "processed_data['PREVAILING_WAGE'] = processed_data['PREVAILING_WAGE'].astype('str')\n",
        "processed_data['PREVAILING_WAGE'] = processed_data.PREVAILING_WAGE.str.replace(',','')\n",
        "processed_data['PREVAILING_WAGE'] = processed_data['PREVAILING_WAGE'].astype('float')\n",
        "#add one column as ANNUAL_SALARY\n",
        "processed_data['ANNUAL_SALARY'] = processed_data['PREVAILING_WAGE'] * processed_data['PW_UNIT_OF_PAY']\n",
        "processed_data = processed_data.drop([\"PREVAILING_WAGE\", \"PW_UNIT_OF_PAY\"], axis=1)\n",
        "counts_soc = processed_data.groupby(\"SOC_CODE\")[\"SOC_CODE\"].transform(len)\n",
        "counts_naics = processed_data.groupby(\"NAICS_CODE\")[\"NAICS_CODE\"].transform(len)\n",
        "counts_worksite_state = processed_data.groupby(\"WORKSITE_STATE\")[\"WORKSITE_STATE\"].transform(len)\n",
        "mask = (counts_soc > 10000) & (counts_naics > 10000)  & (counts_worksite_state > 20000)\n",
        "processed_data = processed_data[mask]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XBPDWNA0SK-",
        "colab_type": "text"
      },
      "source": [
        "### Deal with noise, missing values, numerical and categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRGugNHkGECf",
        "colab_type": "code",
        "outputId": "eca09c07-738a-433f-caf6-e03050583b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "train_set = processed_data.drop([\"CASE_STATUS\"], axis=1)\n",
        "train_labels = processed_data[[\"CASE_STATUS\"]].copy()\n",
        "train_set_num = train_set.drop([\"AGENT_REPRESENTING_EMPLOYER\", \"SOC_CODE\", \"NAICS_CODE\",\n",
        "                                \"FULL_TIME_POSITION\", \"PW_WAGE_LEVEL\", \"H1B_DEPENDENT\", \"WILLFUL_VIOLATOR\",\n",
        "                                \"SUPPORT_H1B\", \"LABOR_CON_AGREE\", \"WORKSITE_STATE\"], axis=1)\n",
        "train_set_cat = train_set.drop([\"TOTAL_WORKERS\",\"NEW_EMPLOYMENT\",\"CONTINUED_EMPLOYMENT\",\n",
        "                                \"CHANGE_PREVIOUS_EMPLOYMENT\", \"NEW_CONCURRENT_EMP\", \"CHANGE_EMPLOYER\",\n",
        "                                \"AMENDED_PETITION\", \"EMP_PERIOD\", \"PROCESS_TIME\",\n",
        "                                \"ANNUAL_SALARY\"], axis=1)\n",
        "#build the pipeline\n",
        "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('std_scaler', StandardScaler()),])\n",
        "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"most_frequent\")),('cat', OneHotEncoder()),])\n",
        "full_pipeline = ColumnTransformer([(\"num\", num_pipeline, list(train_set_num)),(\"cat\", cat_pipeline, list(train_set_cat)),])\n",
        "\n",
        "#prepare the data\n",
        "train_set = full_pipeline.fit_transform(train_set)\n",
        "\n",
        "#prepare the target\n",
        "encoder = LabelEncoder()\n",
        "train_labels = encoder.fit_transform(train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VGv6kw1j2f2",
        "colab_type": "text"
      },
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff-ed4Sp2LDQ",
        "colab_type": "text"
      },
      "source": [
        "Boruta feature selection method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuKN5c2-53z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install Boruta\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from boruta import BorutaPy\n",
        "#Boruta feature selection\n",
        "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
        "\n",
        "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
        "feat_selector.fit(train_set, train_labels)\n",
        "train_set_boruta = feat_selector.transform(train_set)\n",
        "print(\"Dataset with \" + str(train_set.shape[1]) + \" features is reduced to \" + str(train_set_boruta.shape[1])\n",
        "      + \" features after applying Boruta feature selection technique\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2sagoqX2T20",
        "colab_type": "text"
      },
      "source": [
        "L1-based and tree-based feature selection method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVoI2E6c6Tvg",
        "colab_type": "code",
        "outputId": "b6c1f7af-3d3c-4a9c-bbc0-19fa90c18703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "#L1-based feature selection\n",
        "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False, max_iter = 2000).fit(train_set, train_labels)\n",
        "l_model = SelectFromModel(lsvc, prefit=True)\n",
        "train_set_l1 = l_model.transform(train_set)\n",
        "print(\"Dataset with \" + str(train_set.shape[1]) + \" features is reduced to \" + str(train_set_l1.shape[1])\n",
        "      + \" features after applying L1-based feature selection technique\")\n",
        "\n",
        "#tree-based feature selection\n",
        "clf = ExtraTreesClassifier(n_estimators=50)\n",
        "clf = clf.fit(train_set, train_labels)\n",
        "tb_model = SelectFromModel(clf, prefit=True)\n",
        "train_set_tr = tb_model.transform(train_set)\n",
        "print(\"Dataset with \" + str(train_set.shape[1]) + \" features is reduced to \" + str(train_set_tr.shape[1])\n",
        "      + \" features after applying tree-based feature selection technique\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset with 50 features is reduced to 25 features after applying L1-based feature selection technique\n",
            "Dataset with 50 features is reduced to 4 features after applying tree-based feature selection technique\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRBkOssHSlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#store dataset in files using dump\n",
        "import joblib\n",
        "joblib.dump(train_set, PROJECT_ROOT_DIR + 'Data/' + 'train_set' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(train_set_boruta, PROJECT_ROOT_DIR + 'Data/' + 'train_set_boruta' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(train_set_l1, PROJECT_ROOT_DIR + 'Data/' + 'train_set_l1' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(train_set_tr, PROJECT_ROOT_DIR + 'Data/' + 'train_set_tr' + '.gz', compress=('gzip', 3))\n",
        "\n",
        "#store target in files using dump\n",
        "joblib.dump(train_labels, PROJECT_ROOT_DIR + 'Data/' + 'train_labels' + '.gz', compress=('gzip', 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro4TXaI2sUx8",
        "colab_type": "text"
      },
      "source": [
        "### Deal with class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byix8507s90s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import joblib\n",
        "from collections import Counter\n",
        "#load the datasets\n",
        "train_set = joblib.load('/content/drive/My Drive/Data/train_set.gz')\n",
        "train_set_boruta = joblib.load('/content/drive/My Drive/Data/train_set_boruta.gz')\n",
        "train_set_l1 = joblib.load('/content/drive/My Drive/Data/train_set_l1.gz')\n",
        "train_set_tr = joblib.load('/content/drive/My Drive/Data/train_set_tr.gz')\n",
        "#load the labels\n",
        "train_labels = joblib.load('/content/drive/My Drive/Data/train_labels.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrTUV3A8tMC4",
        "colab_type": "text"
      },
      "source": [
        "Rebalance dataset with oversampling technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmCKFngMtGM5",
        "colab_type": "code",
        "outputId": "83eefd8a-2392-4be4-b678-93c91dd94aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "#rebalance the dataset using oversampling (random oversampling)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "ros_train_set_boruta, ros_train_labels_boruta = ros.fit_resample(train_set_boruta, train_labels)\n",
        "print(\"Class distribution of oversampling with train_set_boruta \" + str(sorted(Counter(ros_train_labels_boruta).items())))\n",
        "\n",
        "ros_train_set_l1, ros_train_labels_l1 = ros.fit_resample(train_set_l1, train_labels)\n",
        "print(\"Class distribution of oversampling with train_set_l1 \" + str(sorted(Counter(ros_train_labels_l1).items())))\n",
        "\n",
        "ros_train_set_tr, ros_train_labels_tr = ros.fit_resample(train_set_tr, train_labels)\n",
        "print(\"Class distribution of oversampling with train_set_tr \" + str(sorted(Counter(ros_train_labels_tr).items())))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Class distribution of oversampling with train_set_boruta [(0, 347594), (1, 347594)]\n",
            "Class distribution of oversampling with train_set_l1 [(0, 347594), (1, 347594)]\n",
            "Class distribution of oversampling with train_set_tr [(0, 347594), (1, 347594)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD8r-7TEPfDb",
        "colab_type": "code",
        "outputId": "cd8a5759-8f46-411e-993f-fe4c04b5f034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#store ros_train_set_boruta and ros_train_labels_boruta\n",
        "joblib.dump(ros_train_set_boruta, PROJECT_ROOT_DIR + 'Data/' + 'ros_train_set_boruta' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(ros_train_labels_boruta, PROJECT_ROOT_DIR + 'Data/' + 'ros_train_labels_boruta' + '.gz', compress=('gzip', 3))\n",
        "#store ros_train_set_l1 and ros_train_labels_l1\n",
        "joblib.dump(ros_train_set_l1, PROJECT_ROOT_DIR + 'Data/' + 'ros_train_set_l1' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(ros_train_labels_l1, PROJECT_ROOT_DIR + 'Data/' + 'ros_train_labels_l1' + '.gz', compress=('gzip', 3))\n",
        "#store ros_train_set_tr and ros_train_labels_tr\n",
        "joblib.dump(ros_train_set_tr, PROJECT_ROOT_DIR + 'Data/' + 'ros_train_set_tr' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(ros_train_labels_tr, PROJECT_ROOT_DIR + 'Data/' + 'ros_train_labels_tr' + '.gz', compress=('gzip', 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Data/ros_train_labels_tr.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Coxm_WJwpQh",
        "colab_type": "text"
      },
      "source": [
        "Rebalance dataset with under-sampling technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ypJeXqwuUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
        "#rebalance the dataset using undersampling (nearest neightbours)\n",
        "renn = RepeatedEditedNearestNeighbours()\n",
        "renn_train_set_boruta, renn_train_labels_boruta = renn.fit_resample(train_set_boruta, train_labels)\n",
        "print(\"Class distribution of undersampling with train_set_boruta \" + str(sorted(Counter(renn_train_labels_boruta).items())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C5pGKR8OALJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#store renn_train_set_boruta and renn_train_labels_boruta\n",
        "joblib.dump(renn_train_set_boruta, PROJECT_ROOT_DIR + 'Data/' + 'renn_train_set_boruta' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(renn_train_labels_boruta, PROJECT_ROOT_DIR + 'Data/' + 'renn_train_labels_boruta' + '.gz', compress=('gzip', 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV3Wht_djaxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6bb4aa8-c96a-4f8c-d79a-2249a549159c"
      },
      "source": [
        "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
        "renn = RepeatedEditedNearestNeighbours()\n",
        "renn_train_set_l1, renn_train_labels_l1 = renn.fit_resample(train_set_l1, train_labels)\n",
        "print(\"Class distribution of undersampling with train_set_l1 \" + str(sorted(Counter(renn_train_labels_l1).items())))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class distribution of undersampling with train_set_l1 [(0, 341495), (1, 4044)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPz7N8uzjLAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c3dcb1f-b488-4f36-bc51-20f860065467"
      },
      "source": [
        "#store renn_train_set_l1 and renn_train_labels_l1\n",
        "joblib.dump(renn_train_set_l1, PROJECT_ROOT_DIR + 'Data/' + 'renn_train_set_l1' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(renn_train_labels_l1, PROJECT_ROOT_DIR + 'Data/' + 'renn_train_labels_l1' + '.gz', compress=('gzip', 3))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Data/renn_train_labels_l1.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMcGTs_Yjc6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "renn_train_set_tr, renn_train_labels_tr = renn.fit_resample(train_set_tr, train_labels)\n",
        "print(\"Class distribution of undersampling with train_set_tr \" + str(sorted(Counter(renn_train_labels_tr).items())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PABUAFqjfZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#store renn_train_set_tr and renn_train_labels_tr\n",
        "joblib.dump(renn_train_set_tr, PROJECT_ROOT_DIR + 'Data/' + 'renn_train_set_tr' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(renn_train_labels_tr, PROJECT_ROOT_DIR + 'Data/' + 'renn_train_labels_tr' + '.gz', compress=('gzip', 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x551OsAqwa1L",
        "colab_type": "text"
      },
      "source": [
        "Rebalance dataset with balanced sampling technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B59FGJ7zQNff",
        "colab_type": "code",
        "outputId": "5ec28357-de94-4ba3-adf4-5e78dfb52a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "#rebalance the dataset using balanced sampling (SMOTEENN)\n",
        "smote_enn = SMOTEENN(random_state=0)\n",
        "smote_train_set_boruta, smote_train_labels_boruta = smote_enn.fit_resample(train_set_boruta, train_labels)\n",
        "print(\"Class distribution of balanced sampling with train_set_boruta \" + str(sorted(Counter(smote_train_labels_boruta).items())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class distribution of balanced sampling with train_set_boruta [(0, 324048), (1, 314207)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQgvJxTjWDlf",
        "colab_type": "code",
        "outputId": "52aeb750-1fcb-4b19-ceb2-45444debdb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "smote_train_set_l1, smote_train_labels_l1 = smote_enn.fit_resample(train_set_l1, train_labels)\n",
        "print(\"Class distribution of balanced sampling with train_set_l1 \" + str(sorted(Counter(smote_train_labels_l1).items())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class distribution of balanced sampling with train_set_l1 [(0, 334212), (1, 324659)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dFLYStCtYfX",
        "colab_type": "code",
        "outputId": "51c769f4-2471-4647-a956-dec929688178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "smote_train_set_tr, smote_train_labels_tr = smote_enn.fit_resample(train_set_tr, train_labels)\n",
        "print(\"Class distribution of balanced sampling with smote_train_set_tr \" + str(sorted(Counter(smote_train_labels_tr).items())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class distribution of balanced sampling with train_set_boruta [(0, 315284), (1, 297780)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcEVdpMJv8Gy",
        "colab_type": "code",
        "outputId": "0cbd6cff-ddf6-4c55-fdee-cfccb6ca65df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#store ros_train_set_tr and ros_train_labels_tr\n",
        "joblib.dump(smote_train_set_boruta, PROJECT_ROOT_DIR + 'Data/' + 'smote_train_set_boruta' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(smote_train_labels_boruta, PROJECT_ROOT_DIR + 'Data/' + 'smote_train_labels_boruta' + '.gz', compress=('gzip', 3))\n",
        "\n",
        "#store smote_train_set_l1 and smote_train_labels_l1\n",
        "joblib.dump(smote_train_set_l1, PROJECT_ROOT_DIR + 'Data/' + 'smote_train_set_l1' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(smote_train_labels_l1, PROJECT_ROOT_DIR + 'Data/' + 'smote_train_labels_l1' + '.gz', compress=('gzip', 3))\n",
        "\n",
        "#store smote_train_set_tr and smote_train_labels_tr\n",
        "joblib.dump(smote_train_set_tr, PROJECT_ROOT_DIR + 'Data/' + 'smote_train_set_tr' + '.gz', compress=('gzip', 3))\n",
        "joblib.dump(smote_train_labels_tr, PROJECT_ROOT_DIR + 'Data/' + 'smote_train_labels_tr' + '.gz', compress=('gzip', 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Data/smote_train_labels_tr.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JafooTz4zkTT",
        "colab_type": "text"
      },
      "source": [
        "# Train the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o85_MeINxfSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the 10 datasets\n",
        "#1. original dataset\n",
        "train_set = joblib.load('/content/drive/My Drive/Data/train_set.gz')\n",
        "train_labels = joblib.load('/content/drive/My Drive/Data/train_labels.gz')\n",
        "#2. oversampling dataset with boruta feature selection technique\n",
        "ros_train_set_boruta = joblib.load('/content/drive/My Drive/Data/ros_train_set_boruta.gz')\n",
        "ros_train_labels_boruta = joblib.load('/content/drive/My Drive/Data/ros_train_labels_boruta.gz')\n",
        "#3. oversampling dataset with L1-based feature selection technique\n",
        "ros_train_set_l1 = joblib.load('/content/drive/My Drive/Data/ros_train_set_l1.gz')\n",
        "ros_train_labels_l1 = joblib.load('/content/drive/My Drive/Data/ros_train_labels_l1.gz')\n",
        "#4. oversampling dataset with tree-based feature selection technique\n",
        "ros_train_set_tr = joblib.load('/content/drive/My Drive/Data/ros_train_set_tr.gz')\n",
        "ros_train_labels_tr = joblib.load('/content/drive/My Drive/Data/ros_train_labels_tr.gz')\n",
        "#5. under-sampling dataset with boruta feature selection technique\n",
        "renn_train_set_boruta = joblib.load('/content/drive/My Drive/Data/renn_train_set_boruta.gz')\n",
        "renn_train_labels_boruta = joblib.load('/content/drive/My Drive/Data/renn_train_labels_boruta.gz')\n",
        "#6. under-sampling dataset with L1-based feature selection technique\n",
        "renn_train_set_l1 = joblib.load('/content/drive/My Drive/Data/renn_train_set_l1.gz')\n",
        "renn_train_labels_l1 = joblib.load('/content/drive/My Drive/Data/renn_train_labels_l1.gz')\n",
        "#7. under-sampling dataset with tree-based feature selection technique\n",
        "renn_train_set_tr = joblib.load('/content/drive/My Drive/Data/renn_train_set_tr.gz')\n",
        "renn_train_labels_tr = joblib.load('/content/drive/My Drive/Data/renn_train_labels_tr.gz')\n",
        "#8. balanced sampling dataset with boruta feature selection technique\n",
        "smote_train_set_boruta = joblib.load('/content/drive/My Drive/Data/smote_train_set_boruta.gz')\n",
        "smote_train_labels_boruta = joblib.load('/content/drive/My Drive/Data/smote_train_labels_boruta.gz')\n",
        "#9. balanced sampling dataset with L1-based feature selection technique\n",
        "smote_train_set_l1 = joblib.load('/content/drive/My Drive/Data/smote_train_set_l1.gz') \n",
        "smote_train_labels_l1 = joblib.load('/content/drive/My Drive/Data/smote_train_labels_l1.gz')\n",
        "#10. balanced sampling dataset with tree-based feature selection technique\n",
        "smote_train_set_tr = joblib.load('/content/drive/My Drive/Data/smote_train_set_tr.gz')\n",
        "smote_train_labels_tr = joblib.load('/content/drive/My Drive/Data/smote_train_labels_tr.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u5G1rJAwPjE",
        "colab_type": "text"
      },
      "source": [
        "### Tree models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nPcrLv7pvUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "dt_clf = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wjyeSGgpxsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "524b9270-3567-44a8-a63c-9cde63c4b690"
      },
      "source": [
        "#train the model against the original dataset\n",
        "dt_or_prediction = cross_val_predict(dt_clf.fit(train_set, train_labels), \n",
        "                                           train_set, train_labels, cv=10)\n",
        "#calculate the presion and recall of original dataset\n",
        "dt_or_precision_score = precision_score(train_labels, dt_or_prediction)\n",
        "dt_or_recall_score = recall_score(train_labels, dt_or_prediction)\n",
        "dt_or_accuracy_score = accuracy_score(train_labels, dt_or_prediction)\n",
        "print(\"precision of decision tree model on original dataset is + %f\" % dt_or_precision_score)\n",
        "print(\"recall of decision tree model on original dataset is + %f\" % dt_or_recall_score)\n",
        "print(\"accuracy of decision tree model on original dataset is + %f\" % dt_or_accuracy_score)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on original dataset is + 0.388561\n",
            "recall of decision tree model on original dataset is + 0.618200\n",
            "accuracy of decision tree model on original dataset is + 0.984421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0WwJJ7vp5TG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "dc31a4e8-0c98-45be-88f8-99654581b9c0"
      },
      "source": [
        "#train the model against ros_train_set_boruta, ros_train_labels_boruta\n",
        "dt_ros_boruta_prediction = cross_val_predict(dt_clf.fit(ros_train_set_boruta, ros_train_labels_boruta), \n",
        "                                                       ros_train_set_boruta, ros_train_labels_boruta, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_ros_boruta_prediction\n",
        "dt_ros_boruta_precision_score = precision_score(ros_train_labels_boruta, dt_ros_boruta_prediction)\n",
        "dt_ros_boruta_recall_score = recall_score(ros_train_labels_boruta, dt_ros_boruta_prediction)\n",
        "dt_ros_boruta_accuracy_score = accuracy_score(ros_train_labels_boruta, dt_ros_boruta_prediction)\n",
        "print(\"precision of decision tree model on oversampling and boruta dataset is + %f\" % dt_ros_boruta_precision_score)\n",
        "print(\"recall of decision tree model on oversampling and boruta dataset is + %f\" % dt_ros_boruta_recall_score)\n",
        "print(\"accuracy of decision tree model on oversampling and boruta dataset is + %f\" % dt_ros_boruta_accuracy_score)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on oversampling and boruta dataset is + 0.978310\n",
            "recall of decision tree model on oversampling and boruta dataset is + 0.995662\n",
            "accuracy of decision tree model on oversampling and boruta dataset is + 0.986794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucvZOCSoqO-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2d123701-bf4e-40a9-d5df-e257aec68b35"
      },
      "source": [
        "#train the model against ros_train_set_l1, ros_train_labels_l1\n",
        "dt_ros_l1_prediction = cross_val_predict(dt_clf.fit(ros_train_set_l1, ros_train_labels_l1), \n",
        "                                                   ros_train_set_l1, ros_train_labels_l1, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_ros_l1_prediction\n",
        "dt_ros_l1_precision_score = precision_score(ros_train_labels_l1, dt_ros_l1_prediction)\n",
        "dt_ros_l1_recall_score = recall_score(ros_train_labels_l1, dt_ros_l1_prediction)\n",
        "dt_ros_l1_accuracy_score = accuracy_score(ros_train_labels_l1, dt_ros_l1_prediction)\n",
        "print(\"precision of decision tree model on oversampling and L1-based dataset is + %f\" % dt_ros_l1_precision_score)\n",
        "print(\"recall of decision tree model on oversampling and L1-based dataset is + %f\" % dt_ros_l1_recall_score)\n",
        "print(\"accuracy of decision tree model on oversampling and L1-based dataset is + %f\" % dt_ros_l1_accuracy_score)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on oversampling and L1-based dataset is + 0.981379\n",
            "recall of decision tree model on oversampling and L1-based dataset is + 0.996933\n",
            "accuracy of decision tree model on oversampling and L1-based dataset is + 0.989009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_PZdkdYkHvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "17a06eff-7cd6-4a34-9783-32e93e5522b8"
      },
      "source": [
        "#train the against ros_train_set_tr, ros_train_labels_tr\n",
        "dt_ros_tr_prediction = cross_val_predict(dt_clf.fit(ros_train_set_tr, \n",
        "                                                             ros_train_labels_tr), \n",
        "                                                  ros_train_set_tr, ros_train_labels_tr, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_ros_tr_prediction\n",
        "dt_ros_tr_precision_score = precision_score(ros_train_labels_tr, dt_ros_tr_prediction)\n",
        "dt_ros_tr_recall_score = recall_score(ros_train_labels_tr, dt_ros_tr_prediction)\n",
        "dt_ros_tr_accuracy_score = accuracy_score(ros_train_labels_tr, dt_ros_tr_prediction)\n",
        "print(\"precision of decision tree model on oversampling and tree-based dataset is + %f\" % dt_ros_tr_precision_score)\n",
        "print(\"recall of decision tree model on oversampling and tree-based dataset is + %f\" % dt_ros_tr_recall_score)\n",
        "print(\"accuracy of decision tree model on oversampling and tree-based dataset is + %f\" % dt_ros_tr_accuracy_score)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on oversampling and tree-based dataset is + 0.961970\n",
            "recall of decision tree model on oversampling and tree-based dataset is + 0.980811\n",
            "accuracy of decision tree model on oversampling and tree-based dataset is + 0.971018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfNmV_pwplNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "186da246-6e8b-4c74-c2b5-a3f4a6d8ca8f"
      },
      "source": [
        "#train the model against renn_train_set_boruta, renn_train_labels_boruta\n",
        "dt_renn_boruta_prediction = cross_val_predict(dt_clf.fit(renn_train_set_boruta, renn_train_labels_boruta), \n",
        "                                                       renn_train_set_boruta, renn_train_labels_boruta, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_renn_boruta_prediction\n",
        "dt_renn_boruta_precision_score = precision_score(renn_train_labels_boruta, dt_renn_boruta_prediction)\n",
        "dt_renn_boruta_recall_score = recall_score(renn_train_labels_boruta, dt_renn_boruta_prediction)\n",
        "dt_renn_boruta_accuracy_score = accuracy_score(renn_train_labels_boruta, dt_renn_boruta_prediction)\n",
        "print(\"precision of decision tree model on under-sampling and boruta dataset is + %f\" % dt_renn_boruta_precision_score)\n",
        "print(\"recall of decision tree model on under-sampling and boruta dataset is + %f\" % dt_renn_boruta_recall_score)\n",
        "print(\"accuracy of decision tree model on under-sampling and boruta dataset is + %f\" % dt_renn_boruta_accuracy_score)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on under-sampling and boruta dataset is + 0.501092\n",
            "recall of decision tree model on under-sampling and boruta dataset is + 0.680514\n",
            "accuracy of decision tree model on under-sampling and boruta dataset is + 0.988314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6G9oYJnsaCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7ed73187-64bc-4097-cc11-d574c3a0556e"
      },
      "source": [
        "#train the model against renn_train_set_l1, renn_train_labels_l1\n",
        "dt_renn_l1_prediction = cross_val_predict(dt_clf.fit(renn_train_set_l1, renn_train_labels_l1), \n",
        "                                                   renn_train_set_l1, renn_train_labels_l1, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_renn_l1_prediction\n",
        "dt_renn_l1_precision_score = precision_score(renn_train_labels_l1, dt_renn_l1_prediction)\n",
        "dt_renn_l1_recall_score = recall_score(renn_train_labels_l1, dt_renn_l1_prediction)\n",
        "dt_renn_l1_accuracy_score = accuracy_score(renn_train_labels_l1, dt_renn_l1_prediction)\n",
        "print(\"precision of decision tree model on under-sampling and L1-based dataset is + %f\" % dt_renn_l1_precision_score)\n",
        "print(\"recall of decision tree model on under-sampling and L1-based dataset is + %f\" % dt_renn_l1_recall_score)\n",
        "print(\"accuracy of decision tree model on under-sampling and L1-based dataset is + %f\" % dt_renn_l1_accuracy_score)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on under-sampling and L1-based dataset is + 0.448218\n",
            "recall of decision tree model on under-sampling and L1-based dataset is + 0.662463\n",
            "accuracy of decision tree model on under-sampling and L1-based dataset is + 0.986505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTWS0QI5tD_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "05202560-cfdb-4cb5-9e46-f76bf100bb61"
      },
      "source": [
        "#train the against renn_train_set_tr, ros_train_labels_tr\n",
        "dt_renn_tr_prediction = cross_val_predict(dt_clf.fit(renn_train_set_tr, \n",
        "                                                             renn_train_labels_tr), \n",
        "                                                  renn_train_set_tr, renn_train_labels_tr, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_renn_tr_prediction\n",
        "dt_renn_tr_precision_score = precision_score(renn_train_labels_tr, dt_renn_tr_prediction)\n",
        "dt_renn_tr_recall_score = recall_score(renn_train_labels_tr, dt_renn_tr_prediction)\n",
        "dt_renn_tr_accuracy_score = accuracy_score(renn_train_labels_tr, dt_renn_tr_prediction)\n",
        "print(\"precision of decision tree model on under-sampling and tree-based dataset is + %f\" % dt_renn_tr_precision_score)\n",
        "print(\"recall of decision tree model on under-sampling and tree-based dataset is + %f\" % dt_renn_tr_recall_score)\n",
        "print(\"accuracy of decision tree model on under-sampling and tree-based dataset is + %f\" % dt_renn_tr_accuracy_score)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on under-sampling and tree-based dataset is + 0.541247\n",
            "recall of decision tree model on under-sampling and tree-based dataset is + 0.665183\n",
            "accuracy of decision tree model on under-sampling and tree-based dataset is + 0.989432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA8H3zPjtaHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ba6b6d48-e978-41e5-820d-96ad94a2e73d"
      },
      "source": [
        "#train the model against smote_train_set_boruta, smote_train_labels_boruta\n",
        "dt_smote_boruta_prediction = cross_val_predict(dt_clf.fit(smote_train_set_boruta, smote_train_labels_boruta), \n",
        "                                                       smote_train_set_boruta, smote_train_labels_boruta, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_smote_boruta_prediction\n",
        "dt_smote_boruta_precision_score = precision_score(smote_train_labels_boruta, dt_smote_boruta_prediction)\n",
        "dt_smote_boruta_recall_score = recall_score(smote_train_labels_boruta, dt_smote_boruta_prediction)\n",
        "dt_smote_boruta_accuracy_score = accuracy_score(smote_train_labels_boruta, dt_smote_boruta_prediction)\n",
        "print(\"precision of decision tree model on balanced sampling and boruta dataset is + %f\" % dt_smote_boruta_precision_score)\n",
        "print(\"recall of decision tree model on balanced and boruta dataset is + %f\" % dt_smote_boruta_recall_score)\n",
        "print(\"accuracy of decision tree model on balanced and boruta dataset is + %f\" % dt_smote_boruta_accuracy_score)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on balanced sampling and boruta dataset is + 0.992603\n",
            "recall of decision tree model on balanced and boruta dataset is + 0.996760\n",
            "accuracy of decision tree model on balanced and boruta dataset is + 0.994748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyhpBbUfDEtv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "29e3de5c-b356-4b43-e61e-16f1db8ede1b"
      },
      "source": [
        "#train the model against smote_train_set_l1, smote_train_labels_l1\n",
        "dt_smote_l1_prediction = cross_val_predict(dt_clf.fit(smote_train_set_l1, smote_train_labels_l1), \n",
        "                                                   smote_train_set_l1, smote_train_labels_l1, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_smote_l1_prediction\n",
        "dt_smote_l1_precision_score = precision_score(smote_train_labels_l1, dt_smote_l1_prediction)\n",
        "dt_smote_l1_recall_score = recall_score(smote_train_labels_l1, dt_smote_l1_prediction)\n",
        "dt_smote_l1_accuracy_score = accuracy_score(smote_train_labels_l1, dt_smote_l1_prediction)\n",
        "print(\"precision of decision tree model on balanced sampling and L1-based dataset is + %f\" % dt_smote_l1_precision_score)\n",
        "print(\"recall of decision tree model on balanced sampling and L1-based dataset is + %f\" % dt_smote_l1_recall_score)\n",
        "print(\"accuracy of decision tree model on balanced sampling and L1-based dataset is + %f\" % dt_smote_l1_accuracy_score)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on balanced sampling and L1-based dataset is + 0.990828\n",
            "recall of decision tree model on balanced sampling and L1-based dataset is + 0.996615\n",
            "accuracy of decision tree model on balanced sampling and L1-based dataset is + 0.993786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee-EoLwmD1v_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2474dfc5-1b66-463d-944a-16780e4318a4"
      },
      "source": [
        "#train the against smote_train_set_tr, smote_train_labels_tr\n",
        "dt_smote_tr_prediction = cross_val_predict(dt_clf.fit(smote_train_set_tr, \n",
        "                                                             smote_train_labels_tr), \n",
        "                                                  smote_train_set_tr, smote_train_labels_tr, cv=10)\n",
        "\n",
        "#calculate the presion and recall of dt_smote_tr_prediction\n",
        "dt_smote_tr_precision_score = precision_score(smote_train_labels_tr, dt_smote_tr_prediction)\n",
        "dt_smote_tr_recall_score = recall_score(smote_train_labels_tr, dt_smote_tr_prediction)\n",
        "dt_smote_tr_accuracy_score = accuracy_score(smote_train_labels_tr, dt_smote_tr_prediction)\n",
        "print(\"precision of decision tree model on balanced sampling and tree-based dataset is + %f\" % dt_smote_tr_precision_score)\n",
        "print(\"recall of decision tree model on balanced sampling and tree-based dataset is + %f\" % dt_smote_tr_recall_score)\n",
        "print(\"accuracy of decision tree model on balanced sampling and tree-based dataset is + %f\" % dt_smote_tr_accuracy_score)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision of decision tree model on balanced sampling and tree-based dataset is + 0.991636\n",
            "recall of decision tree model on balanced sampling and tree-based dataset is + 0.995027\n",
            "accuracy of decision tree model on balanced sampling and tree-based dataset is + 0.993508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqFCcgglETvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToG_GdIPEfsC",
        "colab_type": "text"
      },
      "source": [
        "### Linear models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n4TSUe_Et2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "le_clf = SVC(gamma='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGMOr-cGF1om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model against the original dataset\n",
        "le_or_prediction = cross_val_predict(le_clf.fit(train_set, train_labels), \n",
        "                                           train_set, train_labels, cv=10)\n",
        "#calculate the presion and recall of original dataset\n",
        "le_or_precision_score = precision_score(train_labels, dt_or_prediction)\n",
        "le_or_recall_score = recall_score(train_labels, dt_or_prediction)\n",
        "le_or_accuracy_score = accuracy_score(train_labels, dt_or_prediction)\n",
        "print(\"precision of linear model on original dataset is + %f\" % le_or_precision_score)\n",
        "print(\"recall of linear tree model on original dataset is + %f\" % le_or_recall_score)\n",
        "print(\"accuracy of linear tree model on original dataset is + %f\" % le_or_accuracy_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9jygpLxGC_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}